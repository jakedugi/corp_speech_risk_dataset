#!/usr/bin/env python3
"""
Automatically blacklist features that failed comprehensive validation tests.

This script reads the validation results and generates blacklist patterns
for features that failed critical tests (size bias, leakage, etc.).

Usage:
    python scripts/auto_blacklist_failed_features.py \
        --validation-dir docs/feature_development_kfold/iteration_comprehensive_validation_with_6_new \
        --apply-update
"""

import argparse
import csv
import re
from pathlib import Path
from typing import Set, List, Dict, Any


def load_failed_features(validation_dir: Path) -> Dict[str, Set[str]]:
    """Load features that failed various validation tests."""
    failed_features = {
        "size_bias": set(),
        "leakage": set(),
        "extremely_sparse": set(),
        "weak_discrimination": set(),
        "sequence_transitions": set(),  # Sequence transitions are generally problematic
    }

    # Load size bias failures
    size_bias_file = validation_dir / "size_bias_check.csv"
    if size_bias_file.exists():
        with open(size_bias_file, "r") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row.get("size_bias_flag") == "True":
                    feature = row["feature"].replace("interpretable_", "")
                    failed_features["size_bias"].add(feature)

    # Load leakage failures
    leakage_file = validation_dir / "leakage_check.csv"
    if leakage_file.exists():
        with open(leakage_file, "r") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row.get("leakage_flag") == "True":
                    feature = row["feature"].replace("interpretable_", "")
                    failed_features["leakage"].add(feature)

    # Load discriminative power issues (extremely sparse or weak)
    discrim_file = validation_dir / "discriminative_power.csv"
    if discrim_file.exists():
        with open(discrim_file, "r") as f:
            reader = csv.DictReader(f)
            for row in reader:
                feature = row["feature"].replace("interpretable_", "")

                # Extremely sparse features (>99% zeros)
                zero_pct = float(row.get("zero_pct", 0))
                if zero_pct > 99.0:
                    failed_features["extremely_sparse"].add(feature)

                # Weak discrimination (very high p-value)
                kw_pvalue = float(row.get("kw_pvalue", 0))
                if kw_pvalue > 0.5:  # Very weak signal
                    failed_features["weak_discrimination"].add(feature)

                # All sequence transitions (they tend to be problematic)
                if feature.startswith("seq_trans_"):
                    failed_features["sequence_transitions"].add(feature)

    return failed_features


def generate_blacklist_patterns(failed_features: Dict[str, Set[str]]) -> List[str]:
    """Generate regex patterns for blacklisting failed features."""
    patterns = []

    # Add header comment
    patterns.append(
        "# AUTOMATIC BLACKLIST - Features that failed comprehensive validation"
    )
    patterns.append("# Generated by auto_blacklist_failed_features.py")
    patterns.append("")

    # Group by failure reason
    for reason, features in failed_features.items():
        if not features:
            continue

        patterns.append(
            f"# {reason.upper().replace('_', ' ')} - {len(features)} features"
        )

        for feature in sorted(features):
            # Escape special regex characters and create pattern
            escaped_feature = re.escape(feature)
            pattern = f'r"^interpretable\\_{escaped_feature}$",  # {reason}'
            patterns.append(pattern)

        patterns.append("")

    return patterns


def read_column_governance() -> List[str]:
    """Read the current column governance file."""
    governance_file = Path(
        "src/corp_speech_risk_dataset/fully_interpretable/column_governance.py"
    )

    if not governance_file.exists():
        raise FileNotFoundError(f"Column governance file not found: {governance_file}")

    with open(governance_file, "r") as f:
        return f.readlines()


def update_column_governance(new_patterns: List[str], dry_run: bool = True) -> bool:
    """Update the column governance file with new blacklist patterns."""
    governance_file = Path(
        "src/corp_speech_risk_dataset/fully_interpretable/column_governance.py"
    )

    # Read current content
    lines = read_column_governance()

    # Find the BLOCKLIST_PATTERNS section
    blocklist_start = None
    blocklist_end = None

    for i, line in enumerate(lines):
        if "BLOCKLIST_PATTERNS = [" in line:
            blocklist_start = i
        elif blocklist_start is not None and line.strip() == "]":
            blocklist_end = i
            break

    if blocklist_start is None:
        print("❌ Could not find BLOCKLIST_PATTERNS section in column_governance.py")
        return False

    # Check if we already have automatic blacklist section
    has_auto_section = any(
        "AUTOMATIC BLACKLIST" in line for line in lines[blocklist_start:blocklist_end]
    )

    if has_auto_section:
        print(
            "⚠️  Automatic blacklist section already exists. Please remove it manually first."
        )
        return False

    # Insert new patterns before the closing bracket
    insert_pos = blocklist_end

    # Add the new patterns
    new_lines = lines[:insert_pos]

    # Add separator if there are existing patterns
    if insert_pos > blocklist_start + 1:
        new_lines.append("\n")
        new_lines.append("    # " + "=" * 60 + "\n")

    # Add the new patterns with proper indentation
    for pattern in new_patterns:
        if pattern.strip():
            if pattern.startswith("#"):
                new_lines.append(f"    {pattern}\n")
            else:
                new_lines.append(f"    {pattern}\n")
        else:
            new_lines.append("\n")

    # Add the rest of the file
    new_lines.extend(lines[insert_pos:])

    # Write the updated content
    if dry_run:
        print("🔍 DRY RUN - Would add the following patterns:")
        for pattern in new_patterns:
            if pattern.strip():
                print(f"    {pattern}")
        return True
    else:
        with open(governance_file, "w") as f:
            f.writelines(new_lines)
        pattern_count = len([p for p in new_patterns if p.startswith('r"')])
        print(
            f"✅ Updated {governance_file} with {pattern_count} new blacklist patterns"
        )
        return True


def main():
    """Main execution."""
    parser = argparse.ArgumentParser(
        description="Automatically blacklist features that failed validation tests",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )

    parser.add_argument(
        "--validation-dir",
        required=True,
        help="Directory containing validation results",
    )
    parser.add_argument(
        "--apply-update",
        action="store_true",
        help="Actually update column_governance.py (default is dry run)",
    )

    args = parser.parse_args()

    print("🚫 AUTOMATIC FEATURE BLACKLISTING")
    print("=" * 50)
    print(f"Validation directory: {args.validation_dir}")
    print(f"Apply updates: {args.apply_update}")
    print()

    try:
        validation_dir = Path(args.validation_dir)

        if not validation_dir.exists():
            raise FileNotFoundError(f"Validation directory not found: {validation_dir}")

        # Load failed features
        print("📊 Loading validation results...")
        failed_features = load_failed_features(validation_dir)

        # Print summary
        total_failed = sum(len(features) for features in failed_features.values())
        print(f"Found {total_failed} features that failed validation tests:")

        for reason, features in failed_features.items():
            if features:
                print(
                    f"  - {reason.replace('_', ' ').title()}: {len(features)} features"
                )

        if total_failed == 0:
            print("✅ No features failed validation tests!")
            return 0

        # Generate blacklist patterns
        print("\n🔨 Generating blacklist patterns...")
        patterns = generate_blacklist_patterns(failed_features)

        pattern_count = len([p for p in patterns if p.startswith('r"')])
        print(f"Generated {pattern_count} blacklist patterns")

        # Update column governance
        print(
            f"\n📝 {'Updating' if args.apply_update else 'Previewing'} column_governance.py..."
        )
        success = update_column_governance(patterns, dry_run=not args.apply_update)

        if success and not args.apply_update:
            print(
                "\n💡 Run with --apply-update to actually modify column_governance.py"
            )

        return 0 if success else 1

    except Exception as e:
        print(f"❌ Error: {e}")
        import traceback

        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
