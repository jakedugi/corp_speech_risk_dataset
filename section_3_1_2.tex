\subsubsection{3.1.2 Ground truth labeling: quotes, outcomes, and provenance}\label{ground-truth-labeling-quotes-outcomes-and-provenance}

\paragraph{Quotes and attribution}\label{quotes-and-attribution}

Quote detection and attribution followed a multi-sieve pipeline that combined cue-pattern rules with named entity recognition and alias resolution. We used spaCy's EntityRecognizer (ORG/PERSON/MONEY) and integrated an EntityRuler populated with company and executive aliases to canonicalise speakers to a firm-level identifier. Dependency and surface cues (e.g., ``X said'', ``according to Y'') resolved local attributions, then context-window checks and deduplication removed near-duplicates across filings. The canonical mapping and alias inventory are versioned alongside unit tests to prevent regressions. {[}EVID\_SPACY\_DOCS, EVID\_LOC\_NER\_ATTRIBUTION\_CORE{]}

\paragraph{Outcomes and dismissal/bankruptcy handling}\label{outcomes-and-dismissalbankruptcy-handling}

Case outcomes were derived from filings using a multi-pattern amount extractor and a weighted voting scheme. Detection combined regex for currency markers (e.g., ``\$'', ``million/billion''), spaCy MONEY entities, spelled-out numbers, USD prefixes, and fraction patterns. Voting features included proximity to judgment verbs, deception/remedy terms, document type, and chronology, with later-in-docket documents receiving higher weight; thresholds filtered spurious candidates. Dismissals were encoded as 0, bankruptcy outcomes as null, and fee-shifting was flagged separately. Bayesian optimisation over five hyperparameters identified a best run with min\_amount=29310, context\_chars=561, min\_features=15, and late-document thresholds of 0.542/0.795. On a 21-case gold subset, the tuned extractor achieved precision=0.9, recall=0.8, F1=0.85, exact-amount accuracy=0.76, raw coverage=0.95, filtered coverage=0.81, MAE=847329 USD, and RMSE=2156891 USD. Manual verification indicated â‰ˆ10\% residual errors, primarily fee-versus-fund confusions, context-window misses, and some undetected dismissals; we therefore retained dismissal/bankruptcy rules and expose a review flag for borderline small amounts. {[}EVID\_LOC\_OUTCOMES\_VOTING\_CORE, EVID\_LOC\_OUTCOME\_ATTRIBUTION\_EXPERIMENTS, RES-OUTCOME-PREC-20250731, RES-OUTCOME-RECALL-20250731, RES-OUTCOME-F1-20250731, RES-OUTCOME-EXACTACC-20250731, RES-OUTCOME-RAWCOV-20250731, RES-OUTCOME-FILTCOV-20250731, RES-OUTCOME-MAE-20250731, RES-OUTCOME-RMSE-20250731, RESULTS\_MANUAL\_VERIF\_ERROR\_RATE\_10PCT{]}

\paragraph{Provenance, sources, and temporal splits}\label{provenance-sources-and-temporal-splits}

Every quote, attribution, and outcome is linked to a specific CourtListener document and docket via stored provenance fields: courtlistener\_document\_id, docket\_id, source\_url, retrieved\_timestamp, court\_id, court\_name, state, filing\_date, decision\_date, document\_type, case\_number, page\_range, and doc\_sequence\_index. Access occurred through the CourtListener REST API with link-back enforced; we did not redistribute filing PDFs in bulk and release only derived annotations and metadata, consistent with the service's terms. Ground-truth generation and seeds are captured by configuration snapshots in orchestrators/run\_pipeline.py and orchestrators/courtlistener\_orchestrator.py; provenance augmentation is handled by provenance/add\_courtlistener\_provenance.py. Case-wise temporal splits were used to avoid leakage, with an out-of-time holdout recorded in the run configuration and the resulting dataset archived under DOI 10.5281/zenodo.16934610. {[}EVID\_COURTLISTENER\_API{]}
