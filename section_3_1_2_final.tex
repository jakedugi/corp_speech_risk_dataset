\{
``section\_markdown'': ``\#\#\# 3.1.2 Ground truth labeling: quotes, outcomes, and provenance\n\n\#\#\#\# Quotes and attribution\nQuote detection and attribution followed a multi-sieve pipeline that combined cue-pattern rules with named entity recognition and alias resolution. We used spaCy's EntityRecognizer (ORG/PERSON/MONEY) and integrated an EntityRuler populated with company and executive aliases to canonicalise speakers to a firm-level identifier. Dependency and surface cues (e.g., "X said", \textbackslash{}''according to Y\textbackslash{}``) resolved local attributions, then context-window checks and deduplication removed near-duplicates across filings. The canonical mapping and alias inventory are versioned alongside unit tests to prevent regressions. {[}EVID\_SPACY\_DOCS, EVID\_LOC\_NER\_ATTRIBUTION\_CORE{]}\n\n\#\#\#\# Outcomes and dismissal/bankruptcy handling\nCase outcomes were derived from filings using a multi-pattern amount extractor and a weighted voting scheme. Detection combined regex for currency markers (e.g., "\$", \textbackslash{}''million/billion"), spaCy MONEY entities, spelled-out numbers, USD prefixes, and fraction patterns. Voting features included proximity to judgment verbs, deception/remedy terms, document type, and chronology, with later-in-docket documents receiving higher weight; thresholds filtered spurious candidates. Dismissals were encoded as 0, bankruptcy outcomes as null, and fee-shifting was flagged separately. Bayesian optimisation over five hyperparameters identified a best run with min\_amount=29310, context\_chars=561, min\_features=15, and late-document thresholds of 0.542/0.795. On a 21-case gold subset, the tuned extractor achieved precision=0.9, recall=0.8, F1=0.85, exact-amount accuracy=0.76, raw coverage=0.95, filtered coverage=0.81, MAE=847329 USD, and RMSE=2156891 USD. Manual verification indicated ≈10\% residual errors, primarily fee-versus-fund confusions, context-window misses, and some undetected dismissals; we therefore retained dismissal/bankruptcy rules and expose a review flag for borderline small amounts. {[}EVID\_LOC\_OUTCOMES\_VOTING\_CORE, EVID\_LOC\_OUTCOME\_ATTRIBUTION\_EXPERIMENTS, RES-OUTCOME-PREC-20250731, RES-OUTCOME-RECALL-20250731, RES-OUTCOME-F1-20250731, RES-OUTCOME-EXACTACC-20250731, RES-OUTCOME-RAWCOV-20250731, RES-OUTCOME-FILTCOV-20250731, RES-OUTCOME-MAE-20250731, RES-OUTCOME-RMSE-20250731, RESULTS\_MANUAL\_VERIF\_ERROR\_RATE\_10PCT{]}\n\n\#\#\#\# Provenance, sources, and temporal splits\nEvery quote, attribution, and outcome is linked to a specific CourtListener document and docket via stored provenance fields: courtlistener\_document\_id, docket\_id, source\_url, retrieved\_timestamp, court\_id, court\_name, state, filing\_date, decision\_date, document\_type, case\_number, page\_range, and doc\_sequence\_index. Access occurred through the CourtListener REST API with link-back enforced; we did not redistribute filing PDFs in bulk and release only derived annotations and metadata, consistent with the service's terms. Ground-truth generation and seeds are captured by configuration snapshots in orchestrators/run\_pipeline.py and orchestrators/courtlistener\_orchestrator.py; provenance augmentation is handled by provenance/add\_courtlistener\_provenance.py. Case-wise temporal splits were used to avoid leakage, with an out-of-time holdout recorded in the run configuration and the resulting dataset archived under DOI 10.5281/zenodo.16934610. {[}EVID\_COURTLISTENER\_API{]}\n``,''figure\_placeholders'': {[}
``Fig. X: Ground-truth pipeline: (a) quote detection + attribution (NER + multi-sieve) → (b) outcome extraction (multi-pattern + weighted voting) → (c) provenance linking (CourtListener IDs) and temporal split.''{]},
``table\_placeholders'': {[}
``Table X: Outcome extractor performance on the audited subset: precision=0.9; recall=0.8; F1=0.85; exact=0.76; raw\_coverage=0.95; filtered\_coverage=0.81; MAE=847329 USD; RMSE=2156891 USD (n=21).''{]},
``todo\_list'': {[}
``Publish the exact out-of-time anchor date and split manifest used by the pipeline (config hash + seed).'',
``Add a citation entry for CourtListener/RECAP terms of service and confirm redistribution phrasing against the current ToS.'',
``Release the 21-case gold subset identifiers and adjudication notes referenced by the audit.'',
``Version and export the EntityRuler alias list used for attribution, including coverage metrics before/after expansion.'',
``Include an example API request and response schema for the CourtListener endpoints used (dockets, documents).'',
``Document fee-versus-fund disambiguation heuristics and add unit tests targeting observed failure modes.''{]},
``evidence\_map'': {[}
\{
``claim\_id'': ``Q1: Quotes were attributed via multi-sieve rules plus spaCy NER and an EntityRuler alias inventory with canonical speaker mapping.'',
``supporting\_evidence\_ids'': {[}
``EVID\_SPACY\_DOCS'',
``EVID\_LOC\_NER\_ATTRIBUTION\_CORE''{]}
\},
\{
``claim\_id'': ``O1: Outcome extraction used multi-pattern detection and weighted voting with chronology and document-type weights; dismissals=0 and bankruptcy=null.'',
``supporting\_evidence\_ids'': {[}
``EVID\_LOC\_OUTCOMES\_VOTING\_CORE''{]}
\},
\{
``claim\_id'': ``O2: Best-run hyperparameters were min\_amount=29310, context\_chars=561, min\_features=15, thresholds=0.542/0.795 from Bayesian optimisation logs.'',
``supporting\_evidence\_ids'': {[}
``EVID\_LOC\_OUTCOME\_ATTRIBUTION\_EXPERIMENTS''{]}
\},
\{
``claim\_id'': ``O3: Performance on the 21-case audit: precision=0.9, recall=0.8, F1=0.85, exact=0.76, raw\_coverage=0.95, filtered\_coverage=0.81, MAE=847329 USD, RMSE=2156891 USD.'',
``supporting\_evidence\_ids'': {[}
``RES-OUTCOME-PREC-20250731'',
``RES-OUTCOME-RECALL-20250731'',
``RES-OUTCOME-F1-20250731'',
``RES-OUTCOME-EXACTACC-20250731'',
``RES-OUTCOME-RAWCOV-20250731'',
``RES-OUTCOME-FILTCOV-20250731'',
``RES-OUTCOME-MAE-20250731'',
``RES-OUTCOME-RMSE-20250731''{]}
\},
\{
``claim\_id'': ``O4: Manual verification indicated approximately 10\% extraction/labeling error concentrated in fee-vs-fund and dismissal detection.'',
``supporting\_evidence\_ids'': {[}
``RESULTS\_MANUAL\_VERIF\_ERROR\_RATE\_10PCT''{]}
\},
\{
``claim\_id'': ``P1: Provenance fields (document and docket IDs, court identifiers, dates, document type, page ranges, sequence) are stored and link-back is enforced; dataset archived at DOI 10.5281/zenodo.16934610.'',
``supporting\_evidence\_ids'': {[}
``EVID\_COURTLISTENER\_API''{]}
\}{]},
``bib\_entries'': {[}
``Free Law Project. CourtListener RECAP Archive and REST API. https://www.courtlistener.com/help/api/rest/ (accessed 2025-08-23).'',
``spaCy. EntityRecognizer (NER) component API documentation. https://spacy.io/api/entityrecognizer (accessed 2025-08-23).'',
``Zenodo. Quantifying Legal Risk in Corporate Speech with NLP (dataset). DOI: 10.5281/zenodo.16934610.'',
``Internal assets. Quote attribution core and outcome extraction/voting logs (EVID\_LOC\_NER\_ATTRIBUTION\_CORE; EVID\_LOC\_OUTCOMES\_VOTING\_CORE; EVID\_LOC\_OUTCOME\_ATTRIBUTION\_EXPERIMENTS).''{]},
``brief\_decision\_log'': {[}
``Adopted rule-augmented NER for attribution to balance precision with transparent error analysis and alias control.'',
``Used multi-pattern detection plus weighted voting for outcomes to fuse heterogeneous textual signals and chronology.'',
``Encoded dismissals as 0 and bankruptcy as null to separate no-cash outcomes from indeterminate amounts.'',
``Recorded full provenance and enforced link-back via CourtListener IDs to satisfy licensing and auditability.'',
``Captured seeds/configs and archived outputs at a DOI to ensure reproducibility and cross-run comparability.''{]},
``transition\_to\_next'': ``Next we discuss Defendant Quotes.'',
``self\_evaluation'': \{
``publishability\_score'': 8.5,
``improvement\_suggestions'': {[}
``Add the exact OOT anchor date, split manifest, and config hash to complete reproducibility.'',
``Expand alias coverage metrics and show pre/post misattribution rates.'',
``Provide an error taxonomy table with counts from the 21-case audit and targeted fixes.''{]}
\}
\}
