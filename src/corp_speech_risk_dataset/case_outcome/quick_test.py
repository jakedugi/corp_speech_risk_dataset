#!/usr/bin/env python3
"""
quick_test.py

Quick test of the optimization system with minimal dependencies.
"""

import sys
import os
import json
import pandas as pd
from pathlib import Path
from datetime import datetime
import itertools


def test_gold_standard():
    """Test gold standard data loading."""
    print("üß™ Testing gold standard data...")

    try:
        gold_standard_path = (
            "../../../data/gold_standard/case_outcome_amounts_hand_annotated.csv"
        )

        if not os.path.exists(gold_standard_path):
            print(f"‚ùå Gold standard file not found: {gold_standard_path}")
            return False

        df = pd.read_csv(gold_standard_path)
        df = df.dropna(subset=["case_id", "final_amount"])
        df["final_amount"] = pd.to_numeric(df["final_amount"], errors="coerce")
        df = df.dropna(subset=["final_amount"])

        print(f"‚úÖ Gold standard loaded: {len(df)} valid cases")
        print(
            f"   Amount range: ${df['final_amount'].min():,.0f} - ${df['final_amount'].max():,.0f}"
        )

        return True
    except Exception as e:
        print(f"‚ùå Failed to load gold standard: {e}")
        return False


def test_hyperparameter_generation():
    """Test hyperparameter combination generation."""
    print("\nüß™ Testing hyperparameter generation...")

    try:
        # Define a comprehensive hyperparameter grid
        grid = {
            "min_amount": [10000, 50000],
            "context_chars": [100, 200],
            "min_features": [2, 3],
            "case_position_threshold": [0.5],
            "docket_position_threshold": [0.5],
            "fee_shifting_ratio_threshold": [1.0],
            "patent_ratio_threshold": [20.0],
            "dismissal_ratio_threshold": [0.5],
            "bankruptcy_ratio_threshold": [0.5],
            "proximity_pattern_weight": [1.0],
            "judgment_verbs_weight": [1.0],
            "case_position_weight": [1.0],
            "docket_position_weight": [1.0],
            "all_caps_titles_weight": [1.0],
            "document_titles_weight": [1.0],
            "header_chars": [2000],
        }

        # Generate combinations
        keys = grid.keys()
        values = grid.values()
        combinations = []

        for combination in itertools.product(*values):
            hyperparams = dict(zip(keys, combination))
            combinations.append(hyperparams)

        print(f"‚úÖ Generated {len(combinations)} hyperparameter combinations")

        # Show first few combinations
        for i, combo in enumerate(combinations[:3]):
            print(f"   Combination {i+1}: {combo}")

        return len(combinations) > 0

    except Exception as e:
        print(f"‚ùå Failed to generate hyperparameter combinations: {e}")
        return False


def test_optimization_simulation():
    """Simulate optimization process."""
    print("\nüß™ Testing optimization simulation...")

    try:
        # Simulate optimization results
        results = []

        for i in range(5):
            result = {
                "hyperparams": {
                    "min_amount": 10000 + i * 10000,
                    "context_chars": 100 + i * 50,
                    "min_features": 2 + (i % 2),
                    "header_chars": 2000,
                },
                "mse_loss": 1.23e12 + i * 1e11,
                "precision": 0.85 - i * 0.02,
                "recall": 0.80 - i * 0.02,
                "f1_score": 0.824 - i * 0.02,
                "exact_matches": 16 - i,
                "total_cases": 20,
            }
            results.append(result)

        # Sort by MSE loss (lower is better)
        results.sort(key=lambda r: r["mse_loss"])

        print(f"‚úÖ Simulated optimization with {len(results)} results")
        print(f"   Best MSE Loss: {results[0]['mse_loss']:.2e}")
        print(f"   Best F1 Score: {results[0]['f1_score']:.3f}")
        print(f"   Best Precision: {results[0]['precision']:.3f}")
        print(f"   Best Recall: {results[0]['recall']:.3f}")

        return True

    except Exception as e:
        print(f"‚ùå Failed to simulate optimization: {e}")
        return False


def test_progress_tracking():
    """Test progress tracking functionality."""
    print("\nüß™ Testing progress tracking...")

    try:
        total_evaluations = 10
        completed = 0

        for i in range(total_evaluations):
            completed += 1
            progress = (completed / total_evaluations) * 100

            # Simulate evaluation time
            import time

            time.sleep(0.1)

            print(
                f"‚úÖ Evaluation {completed}/{total_evaluations} ({progress:.1f}%) - "
                f"MSE: {1.23e12 + i * 1e11:.2e}, F1: {0.824 - i * 0.02:.3f}"
            )

        print(
            f"‚úÖ Progress tracking completed: {completed}/{total_evaluations} evaluations"
        )
        return True

    except Exception as e:
        print(f"‚ùå Failed to test progress tracking: {e}")
        return False


def test_report_generation():
    """Test report generation functionality."""
    print("\nüß™ Testing report generation...")

    try:
        # Simulate optimization results
        results = []
        for i in range(5):
            result = {
                "hyperparams": {
                    "min_amount": 10000 + i * 10000,
                    "context_chars": 100 + i * 50,
                    "min_features": 2 + (i % 2),
                    "header_chars": 2000,
                },
                "mse_loss": 1.23e12 + i * 1e11,
                "precision": 0.85 - i * 0.02,
                "recall": 0.80 - i * 0.02,
                "f1_score": 0.824 - i * 0.02,
                "exact_matches": 16 - i,
                "total_cases": 20,
            }
            results.append(result)

        # Sort by MSE loss
        results.sort(key=lambda r: r["mse_loss"])

        # Generate report
        print("üìä Optimization Report")
        print("=" * 50)
        print(f"Total evaluations: {len(results)}")
        print(f"Best MSE Loss: {results[0]['mse_loss']:.2e}")
        print(f"Best F1 Score: {results[0]['f1_score']:.3f}")
        print(f"Best Precision: {results[0]['precision']:.3f}")
        print(f"Best Recall: {results[0]['recall']:.3f}")
        print(
            f"Best Exact Matches: {results[0]['exact_matches']}/{results[0]['total_cases']}"
        )

        print("\nüèÜ Top 3 Results:")
        for i, result in enumerate(results[:3]):
            print(
                f"  {i+1}. MSE: {result['mse_loss']:.2e}, F1: {result['f1_score']:.3f}"
            )

        return True

    except Exception as e:
        print(f"‚ùå Failed to generate report: {e}")
        return False


def main():
    """Run all tests."""
    print("üöÄ Running quick optimization system tests...\n")

    tests = [
        ("Gold Standard Loading", test_gold_standard),
        ("Hyperparameter Generation", test_hyperparameter_generation),
        ("Optimization Simulation", test_optimization_simulation),
        ("Progress Tracking", test_progress_tracking),
        ("Report Generation", test_report_generation),
    ]

    passed = 0
    total = len(tests)

    for test_name, test_func in tests:
        print(f"Running: {test_name}")
        if test_func():
            passed += 1
            print(f"‚úÖ {test_name} passed\n")
        else:
            print(f"‚ùå {test_name} failed\n")

    print(f"üìä Test Results: {passed}/{total} tests passed")

    if passed == total:
        print("üéâ All tests passed! Optimization system is ready.")
        print("\nNext steps:")
        print("1. Install scikit-optimize for Bayesian optimization:")
        print("   pip install scikit-optimize")
        print("2. Run quick test: python3 run_optimization.py --max-combinations 5")
        print(
            "3. Run full optimization: python3 run_optimization.py --max-combinations 50"
        )
    else:
        print(
            "‚ö†Ô∏è  Some tests failed. Please check the setup before running optimization."
        )
        sys.exit(1)


if __name__ == "__main__":
    main()
